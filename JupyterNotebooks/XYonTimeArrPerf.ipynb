{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = spark.sparkContext.textFile('file:///C:/Users/Ashwini/Desktop/XYonTimeArrPerf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19914"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file.map(lambda line:line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = file.filter(lambda row:len(row)==4).map(lambda tuple:(tuple[0],tuple[1],tuple[2],float(tuple[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19466"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['127.0.0.1'])  # provide contact points and port\n",
    "session = cluster.connect('aviation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = \"INSERT INTO airport_srcdest_arrival (src, dest, arr_delay) VALUES (?, ?, ?)\"\n",
    "#prepared = session.prepare(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#session.execute(prepared, (row.select('src'), row.select('dest'),row.select('arr_delay')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "rdd3 = file1.map(lambda row: Row(src=row[0], dest=row[1], carrier=row[2], arr_delay=row[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(rdd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+----+---+\n",
      "|          arr_delay|carrier|dest|src|\n",
      "+-------------------+-------+----+---+\n",
      "|               23.0|     AA| ALB|ABE|\n",
      "|  6.266970753957606|     DL| ATL|ABE|\n",
      "| 3.4953071672354947|     EA| ATL|ABE|\n",
      "| 12.214985308521058|     EV| ATL|ABE|\n",
      "|   8.44742857142857|     OH| ATL|ABE|\n",
      "|  5.929155313351498|     EA| AVP|ABE|\n",
      "|                0.0|     EV| AVP|ABE|\n",
      "|               20.5|     OH| AVP|ABE|\n",
      "|-1.6032689450222883|     US| AVP|ABE|\n",
      "|                1.0|     AA| BDL|ABE|\n",
      "|               -3.0|     OO| BHM|ABE|\n",
      "|               61.0|     OH| BWI|ABE|\n",
      "| 10.809917355371901|     PI| BWI|ABE|\n",
      "| 1.2485436893203883|     TW| BWI|ABE|\n",
      "|  4.828444936358605|     US| BWI|ABE|\n",
      "|-2.1604059106284494|     XE| CLE|ABE|\n",
      "| 0.8229916067146283|     US| CLT|ABE|\n",
      "|               6.98|     YV| CLT|ABE|\n",
      "| 2.0990654205607475|     DL| CVG|ABE|\n",
      "|0.20199778024417314|     EV| CVG|ABE|\n",
      "+-------------------+-------+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cassandra-driver in c:\\users\\ashwini\\anaconda3\\lib\\site-packages (3.24.0)\n",
      "Requirement already satisfied: geomet<0.3,>=0.1 in c:\\users\\ashwini\\anaconda3\\lib\\site-packages (from cassandra-driver) (0.2.1.post1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\ashwini\\anaconda3\\lib\\site-packages (from cassandra-driver) (1.14.0)\n",
      "Requirement already satisfied: click in c:\\users\\ashwini\\anaconda3\\lib\\site-packages (from geomet<0.3,>=0.1->cassandra-driver) (7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install cassandra-driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = Cluster(['127.0.0.1'])  # provide contact points and port\n",
    "session = cluster.connect('aviation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rows = session.execute('select * from airport_srcdest_arrival;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(src='APT', dest='SDF', arr_delay=Decimal('-9.8'))\n"
     ]
    }
   ],
   "source": [
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"INSERT INTO airport_srcdest_arrival (src, dest, arr_delay) VALUES (?, ?, ?)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared = session.prepare(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x56800b5ac8>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.execute(prepared, (\"DAD\", \"SDA\", -2.34))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(src='APT', dest='SDF', arr_delay=Decimal('-9.8'))\n",
      "Row(src='DAD', dest='SDA', arr_delay=Decimal('-2.339999999999999857891452847979962825775146484375'))\n"
     ]
    }
   ],
   "source": [
    "rows = session.execute('select * from airport_srcdest_arrival;')\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sqlContext.read \\\n",
    "  .format('org.apache.spark.sql.cassandra') \\\n",
    "  .options(table='airport_srcdest_arrival', keyspace='aviation') \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+\n",
      "|src|dest|           arr_delay|\n",
      "+---+----+--------------------+\n",
      "|ABQ| DFW|4.222648752399232000|\n",
      "|ABQ| DFW|5.601449275362318000|\n",
      "+---+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.write\\\n",
    ".format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".mode('append')\\\n",
    ".options(table = \"airport_src_dest_arrival\", keyspace = \"aviation\")  \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
